## Apresentação

<img src="img/eu.jpeg" width="200" height="270">  
<br><br>

Olá, me chamo **Isabela Fernandes Lopes**, tenho 18 anos e moro em Campo Grande - MS. Me formei Técnica em Informática em 2023 pelo IFMS
e na instituição tive a oportunidade de estagiar e participar de vários projetos que me proporcionaram habilidades técnicas e de trabalho em 
equipe. Atualmente estou cursando o 2° semestre de Engenharia de Software na Universidade Federal de Mato Grosso do Sul, _campus_ Campo Grande.
<br><br>

## Aprendizado

 ### **Sprint 1**
**Linux**  
O intuito de utilizar o Linux como sistema operacional é encorajar o uso do Terminal para agilizar as tarefas realizadas como programador. No curso de Linux da Udemy, utilizei a distribuição Ubuntu, que, além de ser de fácil compreensão mesmo para quem não tem familiaridade com o Linux, é a mais utilizada por servidores web. No curso, aprendi a usar o sistema operacional de forma eficiente, começando com a instalação do Ubuntu no VirtualBox. Aplicando o que foi ensinado nas aulas, me familiarizei com comandos fundamentais para manipulação de arquivos e diretórios, além de gerenciar pacotes e aplicativos usando ferramentas como o apt-get. O curso também incluiu técnicas de busca e filtragem de informações com os comandos find e grep, e a operação com editores de texto como Nano e Vim para edição de arquivos. Além disso, foram abordados o gerenciamento de usuários e grupos, a alteração de permissões e propriedades de arquivos e diretórios, e conceitos básicos de redes para verificar e configurar conexões. Por fim, aprendi a compactar e descompactar arquivos nos formatos .tar.gz e .zip.

**Git e GitHub**  
No curso de Git e GitHub, eu aprendi a gerenciar projetos de código de forma eficaz, começando com a configuração de repositórios e o uso de comandos fundamentais como git init, git add, git commit, e git push para registrar e sincronizar alterações. O curso abordou a criação e junção de branches, o armazenamento temporário de alterações com git stash, e a marcação de pontos importantes no desenvolvimento usando git tag. O curso também traz técnicas para compartilhar e atualizar repositórios, administrar submódulos, e realizar análises com git show e git diff. Ademais, foram discutidas práticas para melhorar commits e o uso do GitHub para gestão de tarefas e documentação com issues, projects, e wiki, além da formatação de documentos em Markdown, linguagem que será muito utilizada durante todo o programa da compass.

___

 ### **Sprint 2**
**SQL**  
No curso de SQL, aprendi a usar comandos fundamentais para manipulação e análise de dados, incluindo SELECT para seleção de colunas, DISTINCT para eliminar duplicatas, e WHERE para filtragem de dados. Explorei a ordenação com ORDER BY, a limitação de resultados com LIMIT, e o uso de operadores aritméticos, de comparação e lógicos. Aprofundei-me em funções agregadas como COUNT() e SUM(), e em agrupamentos com GROUP BY e filtragem de resultados agregados com HAVING. Também aprendi a combinar tabelas com JOIN, unir resultados com UNION, e utilizar subqueries. Aprendi a tratar dados, fazer conversões de unidades, manipulação de strings e datas, e criação de funções personalizadas. Por fim, estudei a manipulação de tabelas e colunas, incluindo criação, modificação e deleção.

**AWS Partner: Sales Accreditation (Business)**  
No curso de AWS, aprendi a promover e vender soluções da AWS de forma eficaz, focando em como identificar e abordar as necessidades dos clientes. O curso abordou a compreensão dos principais serviços e soluções da AWS e a importância de alinhar esses serviços com os requisitos dos clientes para otimizar suas operações e reduzir custos. Também foram discutidas estratégias de vendas e técnicas para construir relacionamentos sólidos com clientes, destacando a importância de uma abordagem consultiva e personalizada. Além disso, o curso ofereceu insights sobre o uso das ferramentas e recursos de suporte da AWS para maximizar o sucesso das vendas e garantir a satisfação do cliente.

___

 ### **Sprint 3**
**Python**  
O curso de Python começou com uma introdução à linguagem e a configuração do ambiente de desenvolvimento. Aprendi a executar código Python e a trabalhar com variáveis, tipos de dados e operadores básicos, também estudei estruturas de controle, como if, for e while, para gerenciar o fluxo do programa. Explorei técnicas de manipulação de arquivos para leitura e escrita de dados, além disso aprofundei-me em comprehensions para criar listas e dicionários de maneira eficiente. O curso cobriu a definição e uso de funções, além de conceitos de programação orientada a objetos, incluindo classes, objetos e herança. Aprendi sobre gerenciamento de pacotes para instalar e manter dependências, e sobre isolamento de ambientes para gerenciar projetos de forma organizada e independente.

**AWS Partner: Cloud Economics**  
O curso AWS Partner: Cloud Economics focou em como maximizar o valor econômico da nuvem para empresas. Comecei aprendendo sobre os princípios fundamentais da economia da nuvem e como a AWS pode ajudar a otimizar custos. Estudei as melhores práticas para calcular o custo total de propriedade (TCO) e analisar os benefícios financeiros da migração para a nuvem. O curso abordou técnicas para avaliar e controlar gastos com recursos, utilizar ferramentas de monitoramento e implementar estratégias de otimização. Também explorei casos de sucesso e estudos de caso que demonstraram como outras empresas reduziram custos e melhoraram a eficiência com a AWS. O treinamento incluiu a compreensão de diferentes modelos de precificação da AWS e a construção de uma estratégia de custo efetiva alinhada aos objetivos de negócios.

___

 ### **Sprint 4**
 **Docker**  
 O curso de Docker começou com uma introdução aos conceitos de containers e à configuração do ambiente. Aprendi a criar e gerenciar containers, explorando como funcionam e como remover ou otimizar suas imagens. Em seguida, estudei volumes para persistir dados e o uso de networks para conectar containers de forma segura. O curso cobriu YAML para configurar containers e Docker Compose, facilitando o gerenciamento de múltiplos containers em aplicações complexas. Também aprendi sobre Docker Swarm para orquestração de clusters e, por fim, uma introdução ao Kubernetes para gerenciar containers em grande escala e automatizar operações de orquestração.

 **Programação funcional com Python**  
 A seção de Programação Funcional com Python apresentou os conceitos principais desse paradigma, como o uso de funções como blocos fundamentais. Aprendi sobre funções de primeira classe, lambdas e funções de alta ordem como map(), filter(), e reduce() para manipular coleções de dados de forma eficiente. Também explorei o uso de imutabilidade e recursão, criando código mais previsível e simplificado, seguindo o estilo funcional de desenvolvimento.

 **AWS Partner: Accreditation (Technical)**  
 O curso de AWS Partner Accreditation (Technical) forneceu uma visão abrangente dos serviços e soluções da AWS. Começou com uma introdução à plataforma AWS, abordando a arquitetura e os principais serviços, como EC2, S3, e RDS. Aprofundei-me em práticas recomendadas para implementar e gerenciar soluções escaláveis e seguras na nuvem. O curso também cobriu tópicos avançados, como automação com AWS Lambda, gestão de identidade e acesso com IAM, e integração de serviços usando APIs e SDKs. Além disso, aprendi sobre estratégias de otimização de custos e monitoramento com CloudWatch, garantindo a eficiência e o desempenho das aplicações na AWS.

**Estatística Descritiva com Python**  
 O curso de Estatística Descritiva com Python na Udemy começou com uma introdução aos conceitos básicos de estatística e à configuração do ambiente de desenvolvimento. Aprofundei-me nos fundamentos da estatística, entendendo como coletar, organizar e interpretar dados. Explorei representações gráficas, como histogramas e gráficos de dispersão, para visualizar e analisar dados. O curso abordou medidas de tendência central, como média, mediana e moda, e medidas de dispersão, incluindo variância e desvio padrão, para descrever a variabilidade dos dados. Também estudei medidas de assimetria para entender a distribuição dos dados. Por fim, apliquei esses conceitos em Data Science, usando Python para realizar análises estatísticas e extrair insights de conjuntos de dados.

___

 ### **Sprint 5**  
**AWS Certified Cloud Practitioner**  
O curso de AWS Certified Cloud Practitioner proporcionou uma compreensão fundamental dos serviços e conceitos da AWS. Iniciei com uma introdução à nuvem e à plataforma AWS, explorando a arquitetura de nuvem e os principais serviços, como EC2, S3 e RDS. O curso abordou os princípios de segurança, gerenciamento e compliance, além de práticas recomendadas para a implementação de soluções eficazes na nuvem. Aprofundei-me em tópicos como o modelo de responsabilidade compartilhada, as ferramentas de gerenciamento de custos e a importância do AWS Well-Architected Framework. Também aprendi sobre as ofertas de suporte da AWS e como aplicar esse conhecimento para auxiliar na tomada de decisões estratégicas em projetos de nuvem. Todo o conhecimento adquirido no curso será utilizado na prova de certificação da AWS 


**AWS Cloud Quest: Cloud Practitioner**  
O jogo AWS Cloud Quest: Cloud Practitioner ofereceu uma experiência interativa e envolvente para aprender sobre os fundamentos da computação em nuvem e os serviços da AWS. Através de uma narrativa imersiva, explorei diversos cenários e desafios que refletem situações do mundo real, permitindo a aplicação prática dos conceitos aprendidos. O jogo abordou tópicos como segurança na nuvem, arquitetura de soluções, e os principais serviços da AWS, como EC2, S3 e DynamoDB. Ao completar as missões, desenvolvi habilidades essenciais para trabalhar com a nuvem, além de uma compreensão mais profunda do ecossistema AWS. Essa abordagem gamificada facilitou o meu aprendizado, tornando o processo de adquirir conhecimento mais dinâmico e divertido.

___

 ### **Sprint 6**  
**Fundamentals of Analytics on AWS**  
O curso de Fundamentals of Analytics on AWS apresentou os principais serviços de análise de dados da AWS, como Amazon S3, AWS Glue, Amazon Athena e Amazon Redshift. Abordou desde a ingestão e processamento de dados até consultas e visualizações interativas com o Amazon QuickSight. Além disso, destacou práticas recomendadas de desempenho, segurança e otimização de custos no gerenciamento de dados na nuvem.

**Serverless Analytics**  
O curso de Serverless Analytics abordou como usar serviços AWS como Lambda, S3, Kinesis e Athena para análises de dados sem gerenciar servidores. Focou em pipelines de dados escaláveis, consultas, e visualizações com Amazon QuickSight, além de destacar eficiência de custos e segurança em soluções serverless.

**Introduction to Amazon Athena**  
O curso de Introduction to Amazon Athena ensinou como usar o Amazon Athena para realizar consultas SQL diretamente em dados armazenados no Amazon S3. Abordou como configurar o Athena, criar e otimizar consultas, além de integrar com outros serviços da AWS. O curso destacou a simplicidade de uso, sem necessidade de infraestrutura, e a eficiência na análise de grandes volumes de dados.

**AWS Glue Getting Started**  
O curso de AWS Glue Getting Started apresentou os fundamentos do AWS Glue para criação de pipelines de ETL (Extração, Transformação e Carga) automatizados. Abordou como configurar jobs, catalogar dados e transformar informações para análises. O curso destacou a facilidade de integração com outros serviços da AWS e a eficiência na preparação de dados em larga escala.

**Amazon EMR Getting Started**  
O curso de Amazon EMR Getting Started abordou como usar o Amazon EMR para processar grandes volumes de dados com frameworks como Apache Hadoop e Spark. Explicou como configurar e gerenciar clusters, executar jobs de análise de dados e otimizar o desempenho. O curso também destacou a escalabilidade e a integração com outros serviços da AWS para processamento eficiente.

**Getting Started with Amazon Redshift**  
O curso de Getting Started with Amazon Redshift ensinou os fundamentos de como configurar e usar o Amazon Redshift para criar data warehouses escaláveis. Abordou a ingestão de dados, consultas SQL eficientes e otimização de desempenho. O curso também destacou práticas para gerenciamento de clusters e integração com outros serviços da AWS para análises avançadas de dados.

**Best Practices for Data Warehousing with Amazon Redshift**  
O curso de Best Practices for Data Warehousing with Amazon Redshift destacou as melhores práticas para otimizar o desempenho do Redshift, incluindo modelagem de dados, particionamento, distribuição e compressão. Também abordou estratégias de monitoramento, segurança e gerenciamento de custos para operações eficazes na plataforma.

**Amazon QuickSight - Getting Started**  
O curso de Amazon QuickSight - Getting Started apresentou os fundamentos do Amazon QuickSight para visualização de dados. Abordou como conectar fontes de dados, criar dashboards interativos e utilizar visualizações gráficas. O curso também destacou a facilidade de uso da ferramenta e suas capacidades de análise em tempo real, permitindo insights rápidos e acessíveis.

___

 ### **Sprint 7**  

 **Spark com Pyspark**  
 O curso de Spark com PySpark começou com uma introdução ao Spark e a preparação do ambiente. Aprendi a manipular grandes volumes de dados usando DataFrames e RDDs, e depois a realizar consultas com Spark SQL, o que facilitou bastante o trabalho com dados estruturados. Em seguida, comecei a criar aplicações práticas e a otimizar o desempenho delas, aprendendo truques e técnicas para tornar os processos mais rápidos e eficientes. Ademais, explorei outros aspectos avançados do Spark, que mostraram como ele pode ser aplicado em várias situações no mundo do Big Data.

 ___

 ### **Sprint 8**  

 **Tutoriais Técnicos**  
 Os tutoriais técnicos da AWS começaram com uma introdução sobre como transformar dados em insights valiosos. Explorei o AWS Glue para catalogar e transformar dados, criando fluxos de trabalho ETL eficientes e escaláveis. Em seguida, aprendi a usar o AWS Athena para consultar grandes volumes de dados diretamente no S3, facilitando a análise sem a necessidade de um banco de dados tradicional. Finalmente, com o Amazon QuickSight, visualizei os dados transformados, criando dashboards interativos que proporcionaram uma visão clara e rápida dos resultados das análises. Cada tutorial mostrou uma parte essencial do ciclo de dados, integrando etapas para manipulação, consulta e visualização.

  ___

 ### **Sprint 9**  
 
 Nesta sprint, aprofundei meus conhecimentos em processamento e transformação de dados utilizando ferramentas da AWS. Iniciei com a modelagem dimensional, estruturando dados para análises eficientes no Data Lake. Em seguida, usei o AWS Glue para criar Jobs que processaram e transformaram dados em fluxos ETL escaláveis, aplicando práticas de integração de dados entre múltiplas fontes. Aprimorei a persistência e catalogação de tabelas no Glue Data Catalog e realizei consultas avançadas no AWS Athena, validando a consistência e qualidade dos dados transformados.
 

 
## Sprints 

1. [Sprint 1](Sprint1/README.md)
2. [Sprint 2](Sprint2/README.md)
3. [Sprint 3](Sprint3/README.md)
4. [Sprint 4](Sprint4/README.md)
5. [Sprint 5](Sprint5/README.md)
6. [Sprint 6](Sprint6/README.md)
7. [Sprint 7](Sprint7/README.md)
8. [Sprint 8](Sprint8/README.md)
9. [Sprint 9](Sprint9/README.md)

___
